{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estonian word embeddings\n",
    "\n",
    "## Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from estnltk import Text # Estonian lemmatization\n",
    "from gensim.models import Word2Vec # main model\n",
    "from gensim.models import KeyedVectors # for loading pre-trained models\n",
    "#from tqdm.notebook import tqdm # progress bar\n",
    "from pathlib import Path # operating system independent file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.9\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpora\\etnc19_balanced_corpus.vert\n",
      "corpora\\etnc19_doaj.vert\n"
     ]
    }
   ],
   "source": [
    "#https://metashare.ut.ee/repository/browse/estonian-national-corpus-2019-vrt-format/be71121e733b11eaa6e4005056b4002483e6e5cdf35343e595e6ba4576d839fb/\n",
    "#NB!!! .VERT files, not .PREVERT\n",
    "\n",
    "corpora_path = Path('./corpora')\n",
    "\n",
    "corpora_names = []\n",
    "\n",
    "for filename in corpora_path.glob('*.vert'):\n",
    "    print(filename)\n",
    "    corpora_names.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpora\\etnc19_balanced_corpus.vert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                      | 986/15010150 [00:00<14:28, 17275.44line/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Mustamäe'], ['ühiselamu'], ['olema'], ['hooneregister'], ['andmed'], ['kokku'], ['neli'], ['omanik'], [','], ['sealhulgas'], ['ka'], ['USA'], ['Oklahoma'], ['osariik'], ['registreerima'], ['Cremo'], ['Capital'], ['L'], ['.'], ['L. C. Ameerika'], ['firma'], ['kui'], ['võimalik'], ['omanik'], ['rääkima'], ['Mustamäe'], ['ühiselamu'], ['initsiatiivgrupp'], ['liige'], ['Jevgenia'], ['Ruzmanova'], ['.'], ['\"'], ['nüüd'], [','], ['kui'], ['linn'], ['olema'], ['koos'], ['jurist'], ['asuma'], ['asi'], ['põhjalikult'], ['uurima'], [','], ['olema'], ['hakkama'], ['ilmnema'], ['huvitav'], ['asjaolu'], ['.'], ['nii'], ['saama'], ['teadma'], [','], ['et'], ['peale'], ['Fennovara'], ['olema'], ['mina'], ['maja'], ['üks'], ['omanik'], ['mingi'], ['Ameerika'], ['firma'], [','], ['\"'], ['rääkima'], ['Ruzmanova'], ['.'], ['Tallinn'], ['sotsiaal'], ['ja'], ['tervishoiuamet'], ['juhataja'], ['Vahur'], ['Keldrima'], ['kinnitama'], ['informatsioon'], ['.'], ['\"'], ['mina'], ['jõudma'], ['kuulduma'], [','], ['et'], ['ühiselamu'], ['olema'], ['tekkima'], ['uus'], ['omanik'], [','], ['kes'], ['olema'], ['üle'], ['võtma'], ['see'], ['omaaegne'], ['Ehitaja'], ['EEV'], ['osa'], [','], ['kuid'], ['see'], ['ei'], ['leidma'], ['kinnitus'], ['.'], ['selguma'], [','], ['et'], ['see'], ['olema'], ['hoopis'], ['teine'], ['firma'], [','], ['\"'], ['ütlema'], ['Keldrima'], ['.'], ['möödunud'], ['aasta'], ['lõpp'], ['üritama'], ['OÜ'], ['Fennovara'], ['müüma'], ['oksjon'], ['ühiselamu'], ['mõtteline'], ['osa'], [','], ['et'], ['tasuma'], ['ühiselamu'], ['müüma'], ['kunagine'], ['Ehitaja'], ['EEV'], ['lubatud'], ['summa'], ['.'], ['kuna'], ['ostusoov'], ['keegi'], ['ei'], ['avaldama'], [','], ['teatama'], ['ühiselamu'], ['üürnik'], [','], ['et'], ['enampakkumine'], ['välja'], ['panema'], ['mõtteline'], ['osa'], ['minema'], ['Ehitaja'], ['EEV'], ['tagasi'], [','], ['mis'], ['hiljem'], ['liituma'], ['firma'], ['Hooldus'], ['ja'], ['Vara'], ['.'], ['hooneregister'], ['ütlema'], [','], ['et'], ['Mustamäe'], ['Akadeemia'], ['38'], [','], ['42'], ['ja'], ['46'], ['ning'], ['Vilde'], ['tee'], ['90'], ['ja'], ['96'], ['ühiselamu'], ['omanik'], ['olema'], ['Tallinn'], ['linn'], [','], ['OÜ'], ['Fennovara'], [','], ['Oklahoma'], ['registreerima'], ['Cremo'], ['Capital'], ['L'], ['.'], ['L'], ['.'], ['C'], ['.'], ['ja'], ['Tallinn'], ['registreerima'], ['OÜ'], ['Beetahansa'], ['.'], ['\"'], ['see'], ['tulema'], ['välja'], [','], ['kui'], ['maa-amet'], ['tegema'], ['päring'], ['võimalik'], ['omanik'], ['või'], ['omanik'], ['kohta'], ['hooneregister'], ['.'], ['kuulus'], ['saama'], ['Fennovara'], ['olema'], ['seal'], ['kiri'], [','], ['aga'], ['Ehitaja'], ['EEV'], ['ega'], ['Vara'], ['ja'], ['Hooldus'], ['mitte'], [','], ['\"'], ['ütlema'], ['Vahur'], ['Keldrima'], ['.']]\n",
      "[['Tallinn'], ['elanik'], ['kommunaalteenus'], ['vahendav'], ['kinnisvarahooldusfirma'], ['BREM'], ['tasuma'], ['AS'], ['Tallinn'], ['vesi'], ['oma'], ['eelmine'], ['aasta'], ['võlg'], ['veeteenus'], ['eest'], [','], ['lähinädal'], ['lootma'], ['BREM'], ['tasuma'], ['võlg'], ['ka'], ['Tallinn'], ['soojus'], ['.'], ['BREM'], ['esindaja'], ['teatama'], [','], ['et'], ['finantskohustus'], ['täitmine'], ['eelmine'], ['aasta'], ['lõpp'], ['seis'], ['Tallinn'], ['vesi'], ['ees'], ['kinnitama'], ['ka'], ['AS'], ['Tallinn'], ['vesi'], ['finantsdirektor'], ['David'], ['Ordman'], ['.'], ['BREM'], ['Haldus'], ['AS'], ['juhatus'], ['liige'], ['Toomas'], ['Õispuu'], ['sõna'], ['toimuma'], ['reede'], ['õhtu'], ['kinnisvarahooldusfirma'], ['ja'], ['Tallinn'], ['vesi'], ['esindaja'], ['vahel'], ['edukas'], ['läbirääkimine'], ['.'], ['\"'], ['sama'], ['päev'], ['õhtu'], ['kandma'], ['raha'], ['üle'], [','], ['\"'], ['märkima'], ['Õispuu'], ['.'], ['\"'], ['mina'], ['tahe'], ['olema'], ['lähinädal'], ['lahendama'], ['ka'], ['võlg'], ['Tallinn'], ['soojus'], [','], ['kes'], ['vastupidiselt'], ['Tallinn'], ['vesi'], ['olema'], ['siiani'], ['käituma'], ['küllaltki'], ['tõrksalt'], ['.'], ['lootma'], ['see'], ['suhtes'], ['abi'], ['ka'], ['Tallinn'], ['linnavalitsus'], [','], ['kes'], ['olema'], ['pidev'], ['dialoog'], [','], ['\"'], ['sõnama'], ['Õispuu'], ['.'], ['1.'], ['jaanuar'], ['seis'], ['ulatuma'], ['BREM'], ['võlg'], ['AS'], ['Tallinn'], ['Soojus'], ['ees'], ['36'], ['miljon'], ['kroon'], ['.'], ['võlg'], ['olema'], ['tekkima'], ['viimane'], ['kolmeeli'], ['kuu'], ['jooksul'], ['.'], ['BREM'], ['võlg'], ['Tallinn'], ['Soojus'], ['ees'], ['olema'], ['tingima'], ['klient'], ['võlgnevus'], [','], ['mis'], ['ulatuma'], ['omakorda'], ['82'], ['miljon'], ['kroon'], ['.'], ['\"'], ['olukord'], ['paranema'], ['vaikselt'], ['ning'], ['mis'], ['kuu'], ['edasi'], [','], ['see'], ['korralikumalt'], ['klient'], ['tasuma'], [','], ['\"'], ['sõnama'], ['Õispuu'], ['.'], ['Eesti'], ['suurim'], ['kinnisvarahooldusfirma'], ['BREM'], ['teenindama'], ['Tallinn'], ['26.000'], ['klient'], ['.'], ['ettevõte'], ['haldama'], ['mitu'], ['tuhat'], ['maja'], ['kokku'], ['üle'], ['1,36'], ['miljon'], ['ruutmeeter'], ['elamispind'], ['.'], ['BREM'], ['koondama'], ['kaheksa'], ['kinnisvarahooldusfirma'], ['Tallinn'], ['eri'], ['linnaosa'], ['.'], ['viimane'], ['pool'], ['aasta'], ['jooksul'], ['olema'], ['BREM'], ['hallatav'], ['elamispind'], ['investeerima'], ['üle'], ['10'], ['miljon'], ['kroon'], ['.'], ['ettevõte'], ['andma'], ['töö'], ['725'], ['inimene'], ['.'], ['Tallinn'], ['elanik'], ['kommunaalteenus'], ['vahendav'], ['kinnisvarahooldusfirma'], ['BREM'], ['tasuma'], ['AS'], ['Tallinn'], ['vesi'], ['oma'], ['eelmine'], ['aasta'], ['võlg'], ['veeteenus'], ['eest'], [','], ['lähinädal'], ['lootma'], ['BREM'], ['tasuma'], ['võlg'], ['ka'], ['Tallinn'], ['soojus'], ['.'], ['BREM'], ['esindaja'], ['teatama'], [','], ['et'], ['finantskohustus'], ['täitmine'], ['eelmine'], ['aasta'], ['lõpp'], ['seis'], ['Tallinn'], ['vesi'], ['ees'], ['kinnitama'], ['ka'], ['AS'], ['Tallinn'], ['vesi'], ['finantsdirektor'], ['David'], ['Ordman'], ['.'], ['BREM'], ['Haldus'], ['AS'], ['juhatus'], ['liige'], ['Toomas'], ['Õispuu'], ['sõna'], ['toimuma'], ['reede'], ['õhtu'], ['kinnisvarahooldusfirma'], ['ja'], ['Tallinn'], ['vesi'], ['esindaja'], ['vahel'], ['edukas'], ['läbirääkimine'], ['.'], ['\"'], ['sama'], ['päev'], ['õhtu'], ['kandma'], ['raha'], ['üle'], [','], ['\"'], ['märkima'], ['Õispuu'], ['.'], ['\"'], ['mina'], ['tahe'], ['olema'], ['lähinädal'], ['lahendama'], ['ka'], ['võlg'], ['Tallinn'], ['soojus'], [','], ['kes'], ['vastupidiselt'], ['Tallinn'], ['vesi'], ['olema'], ['siiani'], ['käituma'], ['küllaltki'], ['tõrksalt'], ['.'], ['lootma'], ['see'], ['suhtes'], ['abi'], ['ka'], ['Tallinn'], ['linnavalitsus'], [','], ['kes'], ['olema'], ['pidev'], ['dialoog'], [','], ['\"'], ['sõnama'], ['Õispuu'], ['.'], ['1.'], ['jaanuar'], ['seis'], ['ulatuma'], ['BREM'], ['võlg'], ['AS'], ['Tallinn'], ['Soojus'], ['ees'], ['36'], ['miljon'], ['kroon'], ['.'], ['võlg'], ['olema'], ['tekkima'], ['viimane'], ['kolmeeli'], ['kuu'], ['jooksul'], ['.'], ['BREM'], ['võlg'], ['Tallinn'], ['Soojus'], ['ees'], ['olema'], ['tingima'], ['klient'], ['võlgnevus'], [','], ['mis'], ['ulatuma'], ['omakorda'], ['82'], ['miljon'], ['kroon'], ['.'], ['\"'], ['olukord'], ['paranema'], ['vaikselt'], ['ning'], ['mis'], ['kuu'], ['edasi'], [','], ['see'], ['korralikumalt'], ['klient'], ['tasuma'], [','], ['\"'], ['sõnama'], ['Õispuu'], ['.'], ['Eesti'], ['suurim'], ['kinnisvarahooldusfirma'], ['BREM'], ['teenindama'], ['Tallinn'], ['26.000'], ['klient'], ['.'], ['ettevõte'], ['haldama'], ['mitu'], ['tuhat'], ['maja'], ['kokku'], ['üle'], ['1,36'], ['miljon'], ['ruutmeeter'], ['elamispind'], ['.'], ['BREM'], ['koondama'], ['kaheksa'], ['kinnisvarahooldusfirma'], ['Tallinn'], ['eri'], ['linnaosa'], ['.'], ['viimane'], ['pool'], ['aasta'], ['jooksul'], ['olema'], ['BREM'], ['hallatav'], ['elamispind'], ['investeerima'], ['üle'], ['10'], ['miljon'], ['kroon'], ['.'], ['ettevõte'], ['andma'], ['töö'], ['725'], ['inimene'], ['.']]\n",
      "[['Briti'], ['naftakompanii'], ['British'], ['Petroleumi'], ['('], ['BP'], [')'], ['kasum'], ['vähenema'], ['naftahind'], ['langus'], ['tõttu'], ['neljas'], ['kvartal'], ['46'], ['protsent'], [','], ['teatama'], ['ettevõte'], ['.'], ['kui'], ['2000.'], ['aasta'], ['neljas'], ['kvartal'], ['teenima'], ['BP'], ['4,09'], ['miljard'], ['dollar'], ['puhaskasum'], [','], ['siis'], ['möödunud'], ['aasta'], ['neljas'], ['kvartal'], ['vastav'], ['näitaja'], ['olema'], ['vaid'], ['2,2'], ['miljard'], ['dollar'], ['.'], ['kogu'], ['möödunud'], ['aasta'], ['puhaskasum'], ['vähenema'], ['miljard'], ['dollar'], ['võrra'], ['ja'], ['moodustama'], ['13,2'], ['miljard'], ['dollar'], ['.'], ['nafta'], ['ja'], ['gaas'], ['tootmine'], ['suurenema'], ['mullu'], ['5,5'], ['protsent'], ['ja'], ['samasugune'], ['juurdekasv'], ['prognoosima'], ['ettevõte'], ['ka'], ['tänavune'], ['aasta'], ['.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/estnltk/estnltk/blob/version_1.6/tutorials/corpus_processing/importing_text_objects_from_corpora.ipynb\n",
    "from estnltk.corpus_processing.parse_enc import parse_enc_file_iterator\n",
    "\n",
    "# input file\n",
    "input_file = corpora_names[0]\n",
    "print(input_file)\n",
    "\n",
    "i = 0\n",
    "\n",
    "all_lemmas = []\n",
    "\n",
    "# iterate over corpus and extract Text objects one-by-one\n",
    "for text in parse_enc_file_iterator(input_file, tokenization=\"preserve_partially\", line_progressbar='ascii',\n",
    "                                    restore_morph_analysis=True): #Add logger\n",
    "    #print(text.layers)\n",
    "    lemmas = text.original_morph_analysis.lemma\n",
    "    #print(text.original_morph_analysis.lemma)\n",
    "    #print((text[\"sentences\"].text))\n",
    "    \n",
    "    all_lemmas.append(lemmas)\n",
    "    \n",
    "    print(lemmas)\n",
    "    \n",
    "    #for lemma in lemmas:\n",
    "        #yield lemma\n",
    "    \n",
    "    \n",
    "    #print(dir(text.text))\n",
    "    \n",
    "    i += 1\n",
    "    if i > 2:\n",
    "        #break\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:word2vec.py:1399: collecting all words and their counts\n",
      "corpora\\etnc19_doaj.vert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                     | 5625/10793527 [00:00<13:21, 13452.07line/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:word2vec.py:1384: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#################################################################| 10793527/10793527 [13:35<00:00, 13229.93line/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:word2vec.py:1407: collected 353499 word types from a corpus of 8014367 raw words and 1595 sentences\n",
      "INFO:word2vec.py:1458: Loading a fresh vocabulary\n",
      "INFO:word2vec.py:1482: effective_min_count=5 retains 68560 unique words (19% of original 353499, drops 284939)\n",
      "INFO:word2vec.py:1488: effective_min_count=5 leaves 7593800 word corpus (94% of original 8014367, drops 420567)\n",
      "INFO:word2vec.py:1547: deleting the raw counts dictionary of 353499 items\n",
      "INFO:word2vec.py:1550: sample=0.001 downsamples 27 most-common words\n",
      "INFO:word2vec.py:1553: downsampling leaves estimated 5894943 word corpus (77.6% of prior 7593800)\n",
      "INFO:base_any2vec.py:1008: estimated required memory for 68560 words and 100 dimensions: 89128000 bytes\n",
      "INFO:word2vec.py:1699: resetting layer weights\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'NoneType' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-7587d00858de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMyCorpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\estonian-embeddings\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sentences, corpus_file, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, max_final_vocab)\u001b[0m\n\u001b[0;32m    598\u001b[0m             \u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m             seed=seed, hs=hs, negative=negative, cbow_mean=cbow_mean, min_alpha=min_alpha, compute_loss=compute_loss)\n\u001b[0m\u001b[0;32m    601\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m     def _do_train_epoch(self, corpus_file, thread_id, offset, cython_vocab, thread_private_mem, cur_epoch,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\estonian-embeddings\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, **kwargs)\u001b[0m\n\u001b[0;32m    743\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"You can't pass a generator as the sentences argument. Try a sequence.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    744\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 745\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    746\u001b[0m             self.train(\n\u001b[0;32m    747\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\estonian-embeddings\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[1;34m(self, sentences, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[0;32m    927\u001b[0m             trim_rule=trim_rule, **kwargs)\n\u001b[0;32m    928\u001b[0m         \u001b[0mreport_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'memory'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimate_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreport_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'num_retained_words'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 929\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainables\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnegative\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbuild_vocab_from_freq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_raw_vocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\estonian-embeddings\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mprepare_weights\u001b[1;34m(self, hs, negative, wv, update, vocabulary)\u001b[0m\n\u001b[0;32m   1685\u001b[0m         \u001b[1;31m# set initial input/projection and hidden weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1686\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1687\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1688\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1689\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\estonian-embeddings\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mreset_weights\u001b[1;34m(self, hs, negative, wv)\u001b[0m\n\u001b[0;32m   1702\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1703\u001b[0m             \u001b[1;31m# construct deterministic seed from word AND seed argument\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1704\u001b[1;33m             \u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseeded_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1705\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1706\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msyn1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer1_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mREAL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'NoneType' and 'str'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class MyCorpus:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        # input file\n",
    "        input_file = corpora_names[1]\n",
    "        print(input_file)\n",
    "\n",
    "        i = 0\n",
    "\n",
    "        #all_lemmas = []\n",
    "\n",
    "        # iterate over corpus and extract Text objects one-by-one\n",
    "        for text in parse_enc_file_iterator(input_file, tokenization=\"preserve_partially\", line_progressbar='ascii',\n",
    "                                            restore_morph_analysis=True): #Add logger\n",
    "            #print(text.layers)\n",
    "            lemmas = text.original_morph_analysis.lemma\n",
    "            doc_lemmas = [x[0] for x in lemmas if x[0] != None]\n",
    "            #print(lemmas)\n",
    "            #print(text.original_morph_analysis.lemma)\n",
    "            #print((text[\"sentences\"].text))\n",
    "\n",
    "            #all_lemmas.append(lemmas)\n",
    "\n",
    "            yield doc_lemmas\n",
    "            \n",
    "import gensim.models\n",
    "\n",
    "sentences = MyCorpus()\n",
    "model = gensim.models.Word2Vec(sentences=sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word2Vec(sentences=None, corpus_file=None, vector_size=100, alpha=0.025, window=5, min_count=5, \n",
    "         max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, \n",
    "         ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, epochs=5, null_word=0, trim_rule=None, \n",
    "         sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), comment=None, max_final_vocab=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-1db80ff8f50d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Tallinn'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.wv['Tallinn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "with tempfile.NamedTemporaryFile(prefix='gensim-model-', delete=False) as tmp:\n",
    "    temporary_filepath = tmp.name\n",
    "    model.save(temporary_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4>Layer</h4>\n",
       "\n",
       "\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>layer name</th>\n",
       "      <th>attributes</th>\n",
       "      <th>parent</th>\n",
       "      <th>enveloping</th>\n",
       "      <th>ambiguous</th>\n",
       "      <th>span count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>words</td>\n",
       "      <td>normalized_form</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "      <th>normalized_form</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Test</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>,</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>kas</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>töötab</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>.</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "Layer(name='words', attributes=('normalized_form',), spans=SL[Span('Test', [{'normalized_form': None}]),\n",
       "Span(',', [{'normalized_form': None}]),\n",
       "Span('kas', [{'normalized_form': None}]),\n",
       "Span('töötab', [{'normalized_form': None}]),\n",
       "Span('.', [{'normalized_form': None}])])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = Text(\"Test, kas töötab.\")\n",
    "text.tag_layer()\n",
    "\n",
    "text.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "sentences = []\n",
    "with open(corpora_names[0], encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if not line.startswith(\"<\"):\n",
    "            #print(line)\n",
    "            sentences.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td><div align = \"left\">Mustamäe ühiselamutel on hooneregistri andmetel kokku neli omanikku, sealhulgas ka USA Oklahoma osariigis registreeritud Cremo Capital L.L.C. Ameerika firmast kui võimalikust omanikust rääkis Mustamäe ühiselamute initsiatiivgrupi liige Jevgenia Ruzmanova.</div></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>layer name</th>\n",
       "      <th>attributes</th>\n",
       "      <th>parent</th>\n",
       "      <th>enveloping</th>\n",
       "      <th>ambiguous</th>\n",
       "      <th>span count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>sentences</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>words</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>tokens</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>compound_tokens</td>\n",
       "      <td>type, normalized</td>\n",
       "      <td>None</td>\n",
       "      <td>tokens</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>words</td>\n",
       "      <td>normalized_form</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>morph_analysis</td>\n",
       "      <td>normalized_text, lemma, root, root_tokens, ending, clitic, form, partofspeech</td>\n",
       "      <td>words</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "Text(text='Mustamäe ühiselamutel on hooneregistri andmetel kokku neli omanikku, sealhulgas ka USA Oklahoma osariigis registreeritud Cremo Capital L.L.C. Ameerika firmast kui võimalikust omanikust rääkis Mustamäe ühiselamute initsiatiivgrupi liige Jevgenia Ruzmanova.')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = Text(sentence)\n",
    "sentence.tag_layer([\"morph_analysis\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mustamäe',\n",
       " 'ühiselamutel',\n",
       " 'on',\n",
       " 'hooneregistri',\n",
       " 'andmetel',\n",
       " 'kokku',\n",
       " 'neli',\n",
       " 'omanikku',\n",
       " ',',\n",
       " 'sealhulgas',\n",
       " 'ka',\n",
       " 'USA',\n",
       " 'Oklahoma',\n",
       " 'osariigis',\n",
       " 'registreeritud',\n",
       " 'Cremo',\n",
       " 'Capital',\n",
       " 'L',\n",
       " '.',\n",
       " 'L.C. Ameerika',\n",
       " 'firmast',\n",
       " 'kui',\n",
       " 'võimalikust',\n",
       " 'omanikust',\n",
       " 'rääkis',\n",
       " 'Mustamäe',\n",
       " 'ühiselamute',\n",
       " 'initsiatiivgrupi',\n",
       " 'liige',\n",
       " 'Jevgenia',\n",
       " 'Ruzmanova',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word for word in sentence.words.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "words = []\n",
    "for sentence in (sentences[:6000]):\n",
    "    sentence = Text(sentence)\n",
    "    sentence.tag_layer([\"morph_analysis\"])\n",
    "    words.append([word for word in sentence.words.text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:word2vec.py:1399: collecting all words and their counts\n",
      "INFO:word2vec.py:1384: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO:word2vec.py:1407: collected 28863 word types from a corpus of 99733 raw words and 6000 sentences\n",
      "INFO:word2vec.py:1458: Loading a fresh vocabulary\n",
      "INFO:word2vec.py:1482: effective_min_count=5 retains 2688 unique words (9% of original 28863, drops 26175)\n",
      "INFO:word2vec.py:1488: effective_min_count=5 leaves 62885 word corpus (63% of original 99733, drops 36848)\n",
      "INFO:word2vec.py:1547: deleting the raw counts dictionary of 28863 items\n",
      "INFO:word2vec.py:1550: sample=0.001 downsamples 33 most-common words\n",
      "INFO:word2vec.py:1553: downsampling leaves estimated 44779 word corpus (71.2% of prior 62885)\n",
      "INFO:base_any2vec.py:1008: estimated required memory for 2688 words and 100 dimensions: 3494400 bytes\n",
      "INFO:word2vec.py:1699: resetting layer weights\n",
      "INFO:base_any2vec.py:1196: training model with 3 workers on 2688 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "INFO:base_any2vec.py:348: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:base_any2vec.py:348: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:base_any2vec.py:348: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:base_any2vec.py:1332: EPOCH - 1 : training on 99733 raw words (44700 effective words) took 0.0s, 1303694 effective words/s\n",
      "INFO:base_any2vec.py:348: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:base_any2vec.py:348: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:base_any2vec.py:348: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:base_any2vec.py:1332: EPOCH - 2 : training on 99733 raw words (44743 effective words) took 0.0s, 1318390 effective words/s\n",
      "INFO:base_any2vec.py:348: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:base_any2vec.py:348: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:base_any2vec.py:348: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:base_any2vec.py:1332: EPOCH - 3 : training on 99733 raw words (44696 effective words) took 0.0s, 1214479 effective words/s\n",
      "INFO:base_any2vec.py:348: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:base_any2vec.py:348: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:base_any2vec.py:348: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:base_any2vec.py:1332: EPOCH - 4 : training on 99733 raw words (44763 effective words) took 0.0s, 1256982 effective words/s\n",
      "INFO:base_any2vec.py:348: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:base_any2vec.py:348: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:base_any2vec.py:348: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:base_any2vec.py:1332: EPOCH - 5 : training on 99733 raw words (44779 effective words) took 0.0s, 1280996 effective words/s\n",
      "INFO:base_any2vec.py:1368: training on a 498665 raw words (223681 effective words) took 0.2s, 1106321 effective words/s\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('oli', 0.9999585747718811),\n",
       " ('Tallinna', 0.9999529123306274),\n",
       " ('kus', 0.9999476671218872),\n",
       " ('poolt', 0.9999476075172424),\n",
       " ('ning', 0.999946653842926),\n",
       " ('vaid', 0.9999442100524902),\n",
       " ('vastu', 0.9999439120292664),\n",
       " ('välja', 0.9999438524246216),\n",
       " ('üle', 0.9999434947967529),\n",
       " ('pärast', 0.9999428391456604)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"Eesti\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
