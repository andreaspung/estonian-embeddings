{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estonian word embeddings\n",
    "\n",
    "## Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.9\n"
     ]
    }
   ],
   "source": [
    "from estnltk import Text # Estonian lemmatization\n",
    "from estnltk.corpus_processing.parse_enc import parse_enc_file_iterator # corpora parsing\n",
    "from gensim.models import Word2Vec # main model\n",
    "from gensim.models import KeyedVectors # for loading pre-trained models\n",
    "from pathlib import Path # operating system independent file paths\n",
    "from platform import python_version\n",
    "import tempfile # for saving model\n",
    "import pickle # for serializing the corpora\n",
    "import csv # for generating csv file\n",
    "from tqdm import tqdm # for progress bar\n",
    "print(python_version()) # Should be 3.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpora\\etnc19_balanced_corpus.vert\n",
      "corpora\\etnc19_doaj.vert\n"
     ]
    }
   ],
   "source": [
    "#https://metashare.ut.ee/repository/browse/estonian-national-corpus-2019-vrt-format/be71121e733b11eaa6e4005056b4002483e6e5cdf35343e595e6ba4576d839fb/\n",
    "#NB!!! .VERT files, not .PREVERT\n",
    "\n",
    "#Place all .vert files to be trained in the folder `corpora`\n",
    "corpora_path = Path('./corpora')\n",
    "\n",
    "corpora_names = []\n",
    "\n",
    "for filename in corpora_path.glob('*.vert'):\n",
    "    print(filename)\n",
    "    corpora_names.append(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating an array of all lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading corpora file corpora\\etnc19_balanced_corpus.vert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|#3                                                                 | 300778/15010150 [00:19<15:54, 15412.17line/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-c2ff5e4d6a50>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m                                         \u001b[0mtokenization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"preserve_partially\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                                         \u001b[0mline_progressbar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ascii'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m                                         restore_morph_analysis=True): #Add logger?\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mlemmas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moriginal_morph_analysis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\estonian-embeddings\\lib\\site-packages\\estnltk\\corpus_processing\\parse_enc.py\u001b[0m in \u001b[0;36mparse_enc_file_iterator\u001b[1;34m(in_file, encoding, focus_doc_ids, focus_srcs, focus_lang, tokenization, original_layer_prefix, restore_morph_analysis, vertParser, textReconstructor, line_progressbar, logger)\u001b[0m\n\u001b[0;32m   1473\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0min_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1474\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_get_iterable_content_w_tqdm\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline_progressbar\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1475\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxmlParser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_next_line\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mline\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1476\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1477\u001b[0m                 \u001b[1;31m# If the parser completed a document and created a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\estonian-embeddings\\lib\\site-packages\\estnltk\\corpus_processing\\parse_enc.py\u001b[0m in \u001b[0;36mparse_next_line\u001b[1;34m(self, line)\u001b[0m\n\u001b[0;32m    994\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextreconstructor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    995\u001b[0m                     \u001b[1;31m# create Text object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 996\u001b[1;33m                     \u001b[0mtext_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextreconstructor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreconstruct_text\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    997\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mtext_obj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    998\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\estonian-embeddings\\lib\\site-packages\\estnltk\\corpus_processing\\parse_enc.py\u001b[0m in \u001b[0;36mreconstruct_text\u001b[1;34m(self, doc_dict)\u001b[0m\n\u001b[0;32m    315\u001b[0m             self._create_original_layers( text, para_locations, sent_locations, \\\n\u001b[0;32m    316\u001b[0m                                           \u001b[0mword_locations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_chunk_locations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 317\u001b[1;33m                                           morph_analyses, s_attribs, par_attribs )\n\u001b[0m\u001b[0;32m    318\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenization\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'estnltk'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m             \u001b[1;31m# Create tokenization with estnltk's default tools\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\estonian-embeddings\\lib\\site-packages\\estnltk\\corpus_processing\\parse_enc.py\u001b[0m in \u001b[0;36m_create_original_layers\u001b[1;34m(self, text_obj, para_locations, sent_locations, word_locations, word_chunk_locations, morph_analyses, sent_extra_attribs, para_extra_attribs, attach_layers)\u001b[0m\n\u001b[0;32m    508\u001b[0m                       \u001b[0mattributes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mWordTagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m                       \u001b[0mtext_object\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtext_obj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 510\u001b[1;33m                       ambiguous=_MAKE_WORDS_AMBIGUOUS).from_records( word_locations )\n\u001b[0m\u001b[0;32m    511\u001b[0m         \u001b[1;31m# Create sentences layer enveloping around words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msent_locations\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent_locations\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\estonian-embeddings\\lib\\site-packages\\estnltk\\layer\\layer.py\u001b[0m in \u001b[0;36mfrom_records\u001b[1;34m(self, records, rewriting)\u001b[0m\n\u001b[0;32m    181\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mrecord\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrecord_line\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m                     \u001b[0mattributes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mattr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mrecord\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mattr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattributes\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_annotation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mElementaryBaseSpan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'start'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecord\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'end'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mattributes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mrecord\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\estonian-embeddings\\lib\\site-packages\\estnltk\\layer\\layer.py\u001b[0m in \u001b[0;36madd_annotation\u001b[1;34m(self, base_span, **attributes)\u001b[0m\n\u001b[0;32m    299\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_span_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_span\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspan\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspan\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSpan\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspan\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 301\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mspan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_annotation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAnnotation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspan\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mattributes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mspan\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\estonian-embeddings\\lib\\site-packages\\estnltk\\layer\\span.py\u001b[0m in \u001b[0;36madd_annotation\u001b[1;34m(self, annotation)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mannotation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspan\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'the annotation has a different span {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mannotation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspan\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mannotation\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattributes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m             raise ValueError('the annotation has unexpected or missing attributes {}!={}'.format(\n\u001b[0;32m     36\u001b[0m                     set(annotation), set(self.layer.attributes)))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# https://github.com/estnltk/estnltk/blob/version_1.6/tutorials/corpus_processing/importing_text_objects_from_corpora.ipynb\n",
    "\n",
    "all_lemmas = []\n",
    "\n",
    "for i in range(len(corpora_names)):\n",
    "    # input file\n",
    "    input_file = corpora_names[i]\n",
    "    print(\"Reading corpora file\", input_file)\n",
    "\n",
    "    # iterate over corpus and extract Text objects one-by-one\n",
    "    for text in parse_enc_file_iterator(input_file, \n",
    "                                        tokenization=\"preserve_partially\", \n",
    "                                        line_progressbar='ascii',\n",
    "                                        restore_morph_analysis=True): #Add logger?\n",
    "\n",
    "        lemmas = text.original_morph_analysis.lemma\n",
    "        all_lemmas.append([lemma[0] for lemma in lemmas if lemma[0] != None])\n",
    "\n",
    "\n",
    "print(len(all_lemmas))\n",
    "print(all_lemmas.count(None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save all the lemmas in a pickled file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all lemmas in a pickled file\n",
    "with open('corpora.pkl', 'wb') as f:\n",
    "    pickle.dump(all_lemmas, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the corpora from the pickled file\n",
    "with open('corpora.pkl', 'rb') as f:\n",
    "    all_lemmas = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Word2vec model\n",
    "\n",
    "Option 1 - training without already pickled lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Maybe that option won't work, let's see\n",
    "class MyCorpus:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        \n",
    "        # input file\n",
    "        \n",
    "        for i in range(len(corpora_names)): # iterate over all corpora files\n",
    "            input_file = corpora_names[i]\n",
    "            print(\"Reading corpora file\", input_file)\n",
    "\n",
    "            # iterate over corpus and extract Text objects one-by-one\n",
    "            for text in parse_enc_file_iterator(input_file, \n",
    "                                                tokenization=\"preserve_partially\", \n",
    "                                                line_progressbar='ascii',\n",
    "                                                restore_morph_analysis=True): #Add logger?\n",
    "\n",
    "                #[['Mustamäe'], ['ühiselamu'], ...\n",
    "                lemmas = text.original_morph_analysis.lemma\n",
    "\n",
    "                #Filter out nonexisting (None) lemmas\n",
    "                # Mustamäe\n",
    "                doc_lemmas = [x[0] for x in lemmas if x[0] != None]\n",
    "\n",
    "                yield doc_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = MyCorpus()\n",
    "model = Word2Vec(sentences=sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 2 - training with already pickled lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the corpora from the pickled file\n",
    "with open('corpora.pkl', 'rb') as f:\n",
    "    all_lemmas = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=all_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:utils.py:551: saving Word2Vec object under models/word2vec.model, separately None\n",
      "INFO:utils.py:657: not storing attribute vectors_norm\n",
      "INFO:utils.py:657: not storing attribute cum_table\n",
      "INFO:utils.py:565: saved models/word2vec.model\n"
     ]
    }
   ],
   "source": [
    "model.save(\"models/word2vec.model\") # for some reason I could not get pathlib to work..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.6329436 , -4.2133846 ,  0.16787282,  4.0586267 ,  1.5857413 ,\n",
       "        0.68046236,  3.3129284 , -2.5557702 , -1.3165423 ,  3.172179  ,\n",
       "       -0.34429678,  0.8267788 , -0.9596452 , -0.48876774,  2.5859125 ,\n",
       "        2.1190176 ,  3.011986  ,  1.5091019 , -1.0206378 ,  0.45206624,\n",
       "        5.0228815 , -2.29621   , -0.24138844,  1.0629926 ,  1.3579892 ,\n",
       "        0.92665577, -4.411577  , -0.7759427 ,  3.4071152 , -0.64127296,\n",
       "       -3.215266  , -1.6152894 ,  1.1413366 , -0.82576925,  1.8092237 ,\n",
       "       -4.341474  , -1.7522033 , -2.0108278 ,  1.575736  ,  4.101202  ,\n",
       "        3.3476107 ,  3.9264216 , -3.0812013 , -4.7063084 ,  2.5465696 ,\n",
       "       -0.75586706,  1.7816662 ,  2.1636212 ,  2.0037787 , -6.004615  ,\n",
       "       -1.7273428 , -0.28177536,  1.062909  , -1.6998087 , -0.21927491,\n",
       "        0.27253053,  1.9296978 ,  0.1006563 , -1.7084851 , -2.089545  ,\n",
       "        1.7609588 ,  2.919195  ,  2.726189  ,  1.3555702 , -1.9165683 ,\n",
       "       -2.7238922 , -0.07057456, -0.62147635,  2.9888675 ,  3.467036  ,\n",
       "       -1.5003519 , -1.541207  , -1.9384276 , -5.2731767 ,  0.23668596,\n",
       "        0.501602  , -3.5418246 , -0.82698846, -0.9237639 , -3.6169143 ,\n",
       "        0.5153642 ,  1.3612468 ,  0.7221486 ,  2.2372584 ,  1.8296882 ,\n",
       "       -1.4003024 , -0.6773688 , -1.7706916 ,  3.4771693 , -2.040628  ,\n",
       "        5.29158   , -1.804989  ,  0.6051433 , -0.12464118,  1.6052656 ,\n",
       "        1.906256  , -0.6883118 ,  1.7760779 , -0.7907955 , -1.3413274 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['Tallinn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:utils.py:431: loading Word2Vec object from model_doaj_5ep\n",
      "INFO:utils.py:465: loading wv recursively from model_doaj_5ep.wv.* with mmap=None\n",
      "INFO:utils.py:503: setting ignored attribute vectors_norm to None\n",
      "INFO:utils.py:465: loading vocabulary recursively from model_doaj_5ep.vocabulary.* with mmap=None\n",
      "INFO:utils.py:465: loading trainables recursively from model_doaj_5ep.trainables.* with mmap=None\n",
      "INFO:utils.py:503: setting ignored attribute cum_table to None\n",
      "INFO:utils.py:437: loaded model_doaj_5ep\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[',', '.', 'olema', 'ja', ')', '(', 'see', ':', '-', '\"']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec.load(\"model_doaj_5ep\")\n",
    "\n",
    "#Most common lemmas\n",
    "model.wv.index2entity[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kollane', 0.9136537313461304),\n",
       " ('must', 0.8914934396743774),\n",
       " ('valge', 0.8838784694671631),\n",
       " ('õis', 0.8835828304290771),\n",
       " ('hall', 0.866241455078125),\n",
       " ('punane', 0.8651580810546875),\n",
       " ('roheline', 0.8572124242782593),\n",
       " ('pruun', 0.8451236486434937),\n",
       " ('luik', 0.8428295850753784),\n",
       " ('vares', 0.8385834693908691)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"sinine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the most popular words in a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:utils.py:431: loading Word2Vec object from model_doaj_5ep\n",
      "INFO:utils.py:465: loading wv recursively from model_doaj_5ep.wv.* with mmap=None\n",
      "INFO:utils.py:503: setting ignored attribute vectors_norm to None\n",
      "INFO:utils.py:465: loading vocabulary recursively from model_doaj_5ep.vocabulary.* with mmap=None\n",
      "INFO:utils.py:465: loading trainables recursively from model_doaj_5ep.trainables.* with mmap=None\n",
      "INFO:utils.py:503: setting ignored attribute cum_table to None\n",
      "INFO:utils.py:437: loaded model_doaj_5ep\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec.load(\"model_doaj_5ep\") #Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = [\"original\"] + list(sum([(\"word\" + str(i), \"score\" + str(i)) for i in range(1, 11)], ()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 10000/10000 [00:31<00:00, 316.36it/s]\n"
     ]
    }
   ],
   "source": [
    "MODEL_FILENAME = \"model_doaj_5ep.csv\"\n",
    "TOPN = 10000 #10 thousand most frequent words\n",
    "with open(MODEL_FILENAME, mode='w', newline='', encoding='utf-8') as model_file:\n",
    "    model_writer = csv.writer(model_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    model_writer.writerow(header)\n",
    "    for common_word in tqdm.tqdm(model.wv.index2entity[:TOPN]):\n",
    "        top_10_similar = model.wv.most_similar(common_word)\n",
    "        top_10_similar_flattened = [common_word] + list(sum(top_10_similar, ()))\n",
    "        model_writer.writerow(top_10_similar_flattened)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
